Brad Smith began his career at Microsoft in 1993. Aside from serving as its president, he is the company’s chief legal officer and the co-author of a new book, “Tools and Weapons: The Promise and the Peril of the Digital Age.” Andrew Ross Sorkin, DealBook editor and columnist for The New York Times, interviewed Mr. Smith at The Times’s first DealBook/DC Strategy Forum. The following excerpts have been edited and condensed.
The world of technology and Silicon Valley now feels like it’s in the cross hairs of Washington and regulators in a way that it has never been before. And the question I have for you is, having lived through the past 10, if not 20, years and the turnaround of Microsoft and everything else, I imagine you get phone calls and you see other executives at rival companies. I’m thinking of Facebook and Apple. I’m thinking of Google and I’m even thinking of your neighbor, Amazon. When you see people like Jeff Bezos in Seattle, what do you tell them? How do you get in front of this? What would you say, given your experience, that they should be doing?
I think that there’s a few things that we’ll always need to think about. I think the first thing you have to step back and think about is: Why is this company in this situation? And to a very large degree it’s because technology has become a tool and a weapon. It is creating rise to so many broad societal concerns. I think the hardest thing for anybody in this position is you have to step back and you have to look yourself in the mirror and not see what you want to see but what other people see. I think you have to acknowledge the problems, because until you acknowledge the problems, you can’t solve them, and nobody is going to believe that you’re trying.
Right now, Silicon Valley is in a very defensive posture. I think that’s probably fair to say. There was an article in The New York Times just this week about whether Apple was placing its own apps higher in the App Store than everybody else’s. Which, by the way, I think you could make the parallel with some of the issues that Microsoft was dealing with when it comes to the web browser war back in the day. To acknowledge up front that, yes, we’ve done this and it’s a problem, does that create more of a problem or not?
It does both. Having lived through it in the ’90s, having been a lawyer, being still a lawyer, at least in part of what I do, you really have to take very carefully all your advice from the lawyers. That’s the first thing I would say because there’s actually a real tension between a statement that a lawyer will say will constitute an admission and saying things that, frankly, everybody knows to be true and puts you on the path to solving a problem. And as I argue inside Microsoft anytime such issues arise, in the long run you’re better served by solving the problem than by trying to win a lawsuit.
Chris Hughes, who’s famous and one of the founders of Facebook, wrote an Op-Ed and is now, I believe, working with the Department of Justice arguing that Facebook should be broken up. And part of his argument isn’t just the monopoly issue, it actually relates to privacy. And I believe that his viewpoint is even broader than just Facebook itself. It’s all of big tech. What do you think of that, well, in terms of the way we look at antitrust in America today?
I think it’s a really important question, and I’m not here or anywhere else advocating that anybody be broken up or sued for that matter. I do think one of the more interesting developments in antitrust law has been people — first from a scholarly perspective and now the head of the Justice Department Antitrust Division — embracing a broader approach. For many years, the prevailing theory was that the only kind of harm that mattered, that was relevant in the world of antitrust, was purely economic harm, which fundamentally tended to relate either to driving people out of business or preventing them from entering or prices rising, with prices being the ultimate test. And this new school of thought, which I think is a logical one, is that actually, given the role of technology in society, we do need to think about the impact on our privacy. We need to think about it in terms of the impact on our democracy perhaps more than anything else. And so there is more in the world of antitrust than economics alone.
You had a meeting with President Obama in the Oval Office in 2013. You write about it. But I think it’s probably worth sharing blow by blow to the room. He effectively said this was coming for Silicon Valley. What happened?
Well, it was a fascinating meeting. It was in December of 2013, just basically a week before the holidays, and it brought together just over a dozen of the tech leaders, the household names people everybody would recognize. And it was all about the Snowden disclosures. And we were there for a very particular purpose, which was to make the case to President Obama, Vice President Biden, the senior staff at the White House, that there needed to be more checks and balances on the data that was in the hands of the National Security Agency.
And you know, as tended to happen, people went around the room and there was a moment when President Obama, in addition to listening very thoughtfully and frankly having issues, made a different point. He said, “I have a suspicion that the guns will turn.” He said, “All of you have more data about people than the government does, and the kinds of demands that you are making on the government when it comes to privacy will eventually be made on you.” I wrote it down because I thought at the time that it was prescient.
And as we say in the book, in a way, Cambridge Analytica in 2018 became the Three Mile Island for data and privacy and technology — meaning suddenly people woke up and almost overnight the political climate changed.
So here’s my next tech question: What do you think it means to be a patriotic company in America today? And the reason I asked this question is because we have a number of big tech companies that have said, we don’t want to do business with the government. And I’m thinking of Google on Maven. You’ve seen the protests at Amazon. You, by the way, are participating in a project with Washington State, I believe, on facial recognition that some people would consider controversial. So where is the line, and when is a company supposed to say, I’m doing this with the government or I’m not?
It is a fundamental question that I don’t think is something one answers with a singular approach to everything. But I can give you a very concrete example that reflects how we thought about it. We asked ourselves what would we provide to the Pentagon, and the answer is everything we make. We feel it is our responsibility as an American company to ensure that people who are literally putting their lives at risk to defend our country have the best technology that we can create for them. But that’s Step One. Step Two is we’ve also said in a democracy, like this one, we have a voice. We have a corporate voice, and it is right that we think about the broader ethical and policy issues that the use of technology in the military can raise. What does it mean to deploy artificial intelligence for weapon systems? These are fundamentally important questions for the future, and so we said we would engage, we would think, we would meet, we’ll be public. And I think that, in our view, is the right approach in the democratic nation.
Let me ask you this: You also write about an experience where an adviser to President Trump talks to you, I think, about using Microsoft to spy elsewhere in the world, to which you say no. What happened?
Well, this relates to another aspect of what it means to be thoughtful in a democratic nation. First of all, we all live and are fully committed, I think, to the rule of law and the protection of certain liberties that are enshrined in our Constitution and the Bill of Rights. And, you know, those rights are not rights that are enjoyed by Americans alone. There are certain fundamental rights, human rights, that, in fact, are enshrined in a global set of principles and even documents that have been endorsed by our own government. So what we’ve also made clear in a wide variety of contexts is, even for our own government, or for that matter for any government, we won’t voluntarily sacrifice the human rights of our customers. We have to put that first above all else.
O.K., but let’s speak to that, because on one end you’re willing to give the technology over to the Pentagon. On the other end, we had the situation with Apple and San Bernardino several years ago and the phone that the government desperately wanted to be opened, and you took the side of Apple.
Well, first of all, I would say what this really points to is two distinct things. There is access to technology, and there is access to people’s data. We create technology. We are prepared to provide it to the United States government. There may be times that if the U.S. uses technology in a way that we think runs afoul of the law, we will use our voice. We sued the Obama administration four times over the surveillance issues. Now, when you get the people’s data, the first principle is — unlike the technology we create — the data that people store in our cloud does not belong to us. We are not the owner of it. It is their data. It is our customers’ data, and our first responsibility in our view is to ensure that their rights and their data are preserved.
Let me make it a little more complicated. You have a founder named Bill Gates who is very anxious about the issue of encryption. He is very worried, talks quite openly and has implied, at least, that he was not necessarily on board with the view of Microsoft when it came to that Apple phone, and he has real concerns that in a world where everything is end-to-end encrypted — when Facebook encrypts its entire system — that actually it’s creating an even bigger problem.
First of all, you’re absolutely right that there’s a variety of views. You know, there’s a thorny set of questions around encryption that will continue to merit thoughtful conversation. There are certain propositions that we would sign up for readily, and then there are aspects for which there should be more conversation. For us, the right answer is not to create back doors. I’ve said before, the path to hell begins at the back door of a software product, because if you create that, you basically undermine people’s security and privacy.
