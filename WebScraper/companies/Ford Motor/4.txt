In a year when the autonomous vehicle business has hit a big reset button, consensus is growing among designers that self-driving cars just aren’t perceptive enough to make them sufficiently safe. So now engineers are considering using more sophisticated sensors and technology — tech that can see through rain and even look underground.
He and others note that the problem is that cars can’t begin to figure out what’s around them — separating toddlers from traffic cones, for example — if they can’t see the objects in the first place. In the parlance of autonomous vehicle engineering, perception has to be accurate enough to enable classification.
Until now, the standard model for autonomous cars has used some combination of four kinds of sensors — video cameras, radar, ultrasonic sensors and lidar. It’s the approach used by all the leading developers in driverless tech, from Aptiv to Waymo. However, as dozens of companies run trials in states like California, deployment dates have drifted, and it has become increasingly obvious that the standard model may not be enough.
Video cameras can be foiled by glare. Standard radar can judge the relative speed of objects but has Mr. Magoo-like vision. Ultrasonic sensors can sense only nearby objects — and not very clearly. Lidar (formally, light detection and ranging), while able to create 3-D images of people and street signs, has distance limitations and can be stymied in heavy rain. And even the most sophisticated artificial intelligence software can’t help if it doesn’t have the perceptual data to begin with.
So to fill in these gaps, companies are considering technologies that seem plucked from a sci-fi movie. From “far infrared” cameras that reveal ghostly images of objects that are invisible to the naked eye to ground-penetrating radar that detects subterranean features, researchers and automakers are increasingly looking at military-derived systems to solve the autonomous puzzle.
A downpour in Las Vegas at the CES technology show a year and a half ago may prove to have been a watershed moment in the race to develop autonomous cars. While other companies promoting their experimental self-driving vehicles had to keep them parked in the rain, one company, AdaSky, demonstrated how its sensors could see people hundreds of feet ahead even in a downpour, and even when they were wearing white and standing against a white background.
AdaSky’s sensors are far infrared cameras (also known as thermal cameras), which detect wavelengths below the visible spectrum that indicate heat. They are particularly good at seeing living things like people or deer in the dark, and have been used in night-vision systems in the past.
Companies like FLIR, AdaSky and Seek Thermal have been developing far infrared camera systems that can see people and objects in extremely challenging conditions. By recognizing even tiny differences in heat signatures, the sensors are like comic book superheroes able to see through souplike fog and blinding smoke, conditions that would stop any current autonomous vehicle in its tracks. These skills are so effective, said Seek Thermal’s chief executive, William J. Parrish, that they are used in fire and rescue operations.
With sensors fitted underneath a vehicle, the radar can create a digital fingerprint of the unique geologic features beneath the pavement, pinpointing where the vehicle is at any time. Other localization solutions, such as lidar-based 3-D maps of what’s above ground, are subject to changing features like construction and weather.
WaveSense’s solution uses radar modules that can see nine to 10 feet underground, well below the frost line. Mr. Bolat also noted that it worked at speeds of up to 70 miles an hour, and the sensors don’t have to be constantly cleaned — unlike optical sensors.
In the lidar race, the technology for autonomous cars is still thought to be in its infancy. Companies like Luminar and Blackmore Sensors and Analytics are working on higher-wavelength models that they believe can provide longer-range, highway-speed systems that see through light rain and snow. (After a $530 million round of funding, Aurora Innovation announced plans to acquire Blackmore in May.)
Radar companies are also working on improvements. Arbe Robotics is developing high-resolution so-called 4-D imaging radar that can create detailed images at distances of over 900 feet. Arbe’s chief executive, Kobi Marenko, said the company was doing field tests now.
“Current sensors that are being used are not good enough yet,” Mr. Marenko acknowledged, referring to conventional Doppler radar. But he said more advanced radar and camera systems could solve the problem without the need for more expensive or offbeat technology.
“The sensor hardware we are using is still a work in progress, especially in terms of achieving mass production and automotive-grade quality,” said Alan Hall, a spokesman for Ford’s Argo AI self-driving car subsidiary, which this summer received a $2.6 billion investment commitment from Volkswagen.
